{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8d5bea4",
   "metadata": {},
   "source": [
    "# StarDist Training Notebook demo, Feb 2024\n",
    "## This notebook will walk you through using the demo model, and how the training works including:\n",
    "* Data Augmentation\n",
    "* Train/Test split\n",
    "* Number of rays\n",
    "* Metrics\n",
    "* Tau\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35cda22",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function, unicode_literals, absolute_import, division\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "matplotlib.rcParams[\"image.interpolation\"] = None\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "from tifffile import imread\n",
    "from csbdeep.utils import Path, normalize, download_and_extract_zip_file\n",
    "\n",
    "\n",
    "from stardist import fill_label_holes, relabel_image_stardist, random_label_cmap, calculate_extents, gputools_available\n",
    "from stardist.matching import matching, matching_dataset\n",
    "from stardist.models import Config2D, StarDist2D, StarDistData2D\n",
    "\n",
    "np.random.seed(42)\n",
    "lbl_cmap = random_label_cmap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b199061",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check to see if GPU is enabled\n",
    "gputools_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337752ee",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "This code block downloads the sample data from the internet, specifically the github repository from the authors\n",
    "\n",
    "### When formatting your data for training, pay careful attention to how the data is formatted in the sample case. You will want to name your images in the same manner.\n",
    "\n",
    "### Note! Labels that are made by humans are typically called the \"ground truth\"\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "Training data (for input `X` with associated label masks `Y`) can be provided via lists of numpy arrays, where each image can have a different size. Alternatively, a single numpy array can also be used if all images have the same size.  \n",
    "Input images can either be two-dimensional (single-channel) or three-dimensional (multi-channel) arrays, where the channel axis comes last. Label images need to be integer-valued.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284e1af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "download_and_extract_zip_file(\n",
    "    url       = 'https://github.com/stardist/stardist/releases/download/0.1.0/dsb2018.zip',\n",
    "    targetdir = 'data',\n",
    "    verbose   = 1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1032b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = sorted(glob('data/dsb2018/train/images/*.tif'))\n",
    "Y = sorted(glob('data/dsb2018/train/masks/*.tif'))\n",
    "assert all(Path(x).name==Path(y).name for x,y in zip(X,Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c647be9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The first three label images are: ',*X[0:3], sep='\\n')\n",
    "print('')\n",
    "print('The first three label images are: ',*Y[0:3], sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cedf5559",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = list(map(imread,X))\n",
    "Y = list(map(imread,Y))\n",
    "n_channel = 1 if X[0].ndim == 2 else X[0].shape[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4fb3e4",
   "metadata": {},
   "source": [
    "Normalize images and fill small label holes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519c0152",
   "metadata": {},
   "outputs": [],
   "source": [
    "axis_norm = (0,1)   # normalize channels independently\n",
    "# axis_norm = (0,1,2) # normalize channels jointly\n",
    "if n_channel > 1:\n",
    "    print(\"Normalizing image channels %s.\" % ('jointly' if axis_norm is None or 2 in axis_norm else 'independently'))\n",
    "    sys.stdout.flush()\n",
    "\n",
    "X = [normalize(x,1,99.8,axis=axis_norm) for x in tqdm(X)]\n",
    "Y = [fill_label_holes(y) for y in tqdm(Y)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3ae039",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "assert len(X) > 1, \"not enough training data\"\n",
    "rng = np.random.RandomState(42)\n",
    "ind = rng.permutation(len(X))\n",
    "n_val = max(1, int(round(0.15 * len(ind))))\n",
    "ind_train, ind_val = ind[:-n_val], ind[-n_val:]\n",
    "X_val, Y_val = [X[i] for i in ind_val]  , [Y[i] for i in ind_val]\n",
    "X_trn, Y_trn = [X[i] for i in ind_train], [Y[i] for i in ind_train] \n",
    "print('number of images: %3d' % len(X))\n",
    "print('- training:       %3d' % len(X_trn))\n",
    "print('- validation:     %3d' % len(X_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a106ee6",
   "metadata": {},
   "source": [
    "### Training data consists of pairs of input image and label instances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0298ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_img_label(img, lbl, img_title=\"image\", lbl_title=\"label\", **kwargs):\n",
    "    fig, (ai,al) = plt.subplots(1,2, figsize=(12,5), gridspec_kw=dict(width_ratios=(1.25,1)))\n",
    "    im = ai.imshow(img, cmap='gray', clim=(0,1))\n",
    "    ai.set_title(img_title)    \n",
    "    fig.colorbar(im, ax=ai)\n",
    "    \n",
    "    al.imshow(lbl, cmap=lbl_cmap)\n",
    "    al.set_title(lbl_title)\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44940e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = min(9, len(X)-1)\n",
    "img, lbl = X[i], Y[i]\n",
    "assert img.ndim in (2,3)\n",
    "img = img if (img.ndim==2 or img.shape[-1]==3) else img[...,0]\n",
    "plot_img_label(img,lbl)\n",
    "None;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ceeec7a",
   "metadata": {},
   "source": [
    "# Configuration\n",
    "\n",
    "A `StarDist2D` model is specified via a `Config2D` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8b2b991",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Config2D.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76d65bc",
   "metadata": {},
   "source": [
    "# Example image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4c6196",
   "metadata": {},
   "source": [
    "# See an image with it's ground truth labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a05f350",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Make up list of images and their labels\n",
    "\n",
    "i = min(4, len(X)-1)\n",
    "img, lbl = X[i], fill_label_holes(Y[i])\n",
    "assert img.ndim in (2,3)\n",
    "img = img if img.ndim==2 else img[...,:3]\n",
    "# assumed axes ordering of img and lbl is: YX(C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e901e5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the image\n",
    "\n",
    "plt.figure(figsize=(16,10))\n",
    "plt.subplot(121); plt.imshow(img,cmap='gray');   plt.axis('off'); plt.title('Raw image')\n",
    "plt.subplot(122); plt.imshow(lbl,cmap=lbl_cmap); plt.axis('off'); plt.title('GT labels')\n",
    "None;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "531a1876",
   "metadata": {},
   "source": [
    "# Fitting ground-truth labels with star-convex polygons\n",
    "\n",
    "## Rays\n",
    "### StarDist uses rays from a central point to map out the perimeter of the object.  The more rays coming out, the closer to actual the perimeter will be.  There is a compromise to be made though, the more rays you have, the slower the training will be\n",
    "\n",
    "#### This will show us if the objects can be fit by star-convex polygons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435cc29d",
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "## Generate images with different number of rays from 2^1 to 2^8\n",
    "\n",
    "n_rays = [2**i for i in range(2,8)]\n",
    "scores = []\n",
    "for r in tqdm(n_rays):\n",
    "    Y_reconstructed = [relabel_image_stardist(lbl, n_rays=r) for lbl in Y]\n",
    "    mean_iou = matching_dataset(Y, Y_reconstructed, thresh=0, show_progress=False).mean_true_score\n",
    "    scores.append(mean_iou)"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "ec2c8a3e",
   "metadata": {},
   "source": [
    "## Intersection over union\n",
    " The traditional measure of the how well a label fits to a ground truth is by measuring the intersection over union. \n",
    " ![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99665d4c",
   "metadata": {},
   "source": [
    "## Here we can see how well we reconstruct the ground truth labels with varying amount of rays.  Obviously four is too few rays, but as we increase the number of rays after a certain point, we get diminishing returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa518cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2,3, figsize=(16,11))\n",
    "for a,r in zip(ax.flat,n_rays):\n",
    "    a.imshow(relabel_image_stardist(lbl, n_rays=r), cmap=lbl_cmap)\n",
    "    a.set_title('Reconstructed (%d rays)' % r)\n",
    "    a.axis('off')\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8553d10d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(n_rays, scores, 'o-')\n",
    "plt.xlabel('Number of rays for star-convex polygon')\n",
    "plt.ylabel('Reconstruction score (mean intersection over union)')\n",
    "plt.title(\"Accuracy of ground truth reconstruction (should be > 0.8 for a reasonable number of rays)\")\n",
    "None;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13c9de7",
   "metadata": {},
   "source": [
    "# Do the Training!\n",
    "### Below is the different variables we can use to do some training. \n",
    "\n",
    "* Alter the number of rays based on how your images look\n",
    "* If you have memory issues, change the grid size (but default is generally ok)\n",
    "* Change use_gpu to True if you want to use the GPU for training (you do)\n",
    "* There are a ton of other variables you can use.  One thing that is often done is transfer learning, where you can use a pre-built model and add data to it to increase it's accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dcc81f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 32 is a good default choice (see 1_data.ipynb)\n",
    "n_rays = 32  #number of rays\n",
    "\n",
    "# Use OpenCL-based computations for data generator during training (requires 'gputools')\n",
    "use_gpu = True and gputools_available()\n",
    "\n",
    "# Predict on subsampled grid for increased efficiency and larger field of view\n",
    "grid = (2,2)\n",
    "\n",
    "conf = Config2D (\n",
    "    n_rays       = n_rays,\n",
    "    grid         = grid,\n",
    "    use_gpu      = use_gpu,\n",
    "    n_channel_in = n_channel,\n",
    ")\n",
    "print(conf)\n",
    "vars(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb796c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_gpu:\n",
    "    from csbdeep.utils.tf import limit_gpu_memory\n",
    "    # adjust as necessary: limit GPU memory to be used by TensorFlow to leave some to OpenCL-based computations\n",
    "    #limit_gpu_memory(0.8)\n",
    "    # alternatively, try this:\n",
    "    limit_gpu_memory(None, allow_growth=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1836c4",
   "metadata": {},
   "source": [
    "**Note:** The trained `StarDist2D` model will *not* predict completed shapes for partially visible objects at the image boundary if `train_shape_completion=False` (which is the default option)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ade7f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = StarDist2D(conf, name='stardist_demo', basedir='models')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72483ad8",
   "metadata": {},
   "source": [
    "Check if the neural network has a large enough field of view to see up to the boundary of most objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36524675",
   "metadata": {},
   "outputs": [],
   "source": [
    "median_size = calculate_extents(list(Y), np.median)\n",
    "fov = np.array(model._axes_tile_overlap('YX'))\n",
    "print(f\"median object size:      {median_size}\")\n",
    "print(f\"network field of view :  {fov}\")\n",
    "if any(median_size > fov):\n",
    "    print(\"WARNING: median object size larger than field of view of the neural network.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3cce3c3",
   "metadata": {},
   "source": [
    "# Data Augmentation\n",
    "You can define a function/callable that applies augmentation to each batch of the data generator.  \n",
    "We here use an `augmenter` that applies random rotations, flips, and intensity changes, which are typically sensible for (2D) microscopy images (but you can disable augmentation by setting `augmenter = None`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab32c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_fliprot(img, mask): \n",
    "    assert img.ndim >= mask.ndim\n",
    "    axes = tuple(range(mask.ndim))\n",
    "    perm = tuple(np.random.permutation(axes))\n",
    "    img = img.transpose(perm + tuple(range(mask.ndim, img.ndim))) \n",
    "    mask = mask.transpose(perm) \n",
    "    for ax in axes: \n",
    "        if np.random.rand() > 0.5:\n",
    "            img = np.flip(img, axis=ax)\n",
    "            mask = np.flip(mask, axis=ax)\n",
    "    return img, mask \n",
    "\n",
    "def random_intensity_change(img):\n",
    "    img = img*np.random.uniform(0.6,2) + np.random.uniform(-0.2,0.2)\n",
    "    return img\n",
    "\n",
    "\n",
    "def augmenter(x, y):\n",
    "    \"\"\"Augmentation of a single input/label image pair.\n",
    "    x is an input image\n",
    "    y is the corresponding ground-truth label image\n",
    "    \"\"\"\n",
    "    x, y = random_fliprot(x, y)\n",
    "    x = random_intensity_change(x)\n",
    "    # add some gaussian noise\n",
    "    sig = 0.02*np.random.uniform(0,1)\n",
    "    x = x + sig*np.random.normal(0,1,x.shape)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10536e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot some augmented examples\n",
    "img, lbl = X[0],Y[0]\n",
    "plot_img_label(img, lbl)\n",
    "for _ in range(3):\n",
    "    img_aug, lbl_aug = augmenter(img,lbl)\n",
    "    plot_img_label(img_aug, lbl_aug, img_title=\"image augmented\", lbl_title=\"label augmented\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39600627",
   "metadata": {},
   "source": [
    "# Callbacks\n",
    "\n",
    "Callbacks are functions that you can specify to run during the training, to get a feel for what is going on in the training.  Here, I wrote a Callback that every tenth epoch runs the model \"as is\" on a validation image, and generates a label image, and saves it to an image path.\n",
    "\n",
    "There are actually two images generated, a label image, and an image with the labels overlaid on the original image.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90abf2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import Callback\n",
    "\n",
    "\n",
    "class EvaluateModelCallback(Callback):\n",
    "    def __init__(self, validation_data, im_path):\n",
    "        super(EvaluateModelCallback, self).__init__()\n",
    "        self.validation_data = validation_data\n",
    "        self.im_path = im_path\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if epoch % 10 == 0:\n",
    "            # Assuming you have a function to evaluate your model on validation data\n",
    "            Y_val_pred = model.predict_instances(validation_data, n_tiles=model._guess_n_tiles(validation_data), show_tile_progress=False)[0]\n",
    "            \n",
    "            # Print or log the evaluation results\n",
    "            print(f\"\\nEvaluation results at epoch {epoch}:\")\n",
    "            plt.imshow(validation_data, cmap='gray')\n",
    "            plt.imshow(Y_val_pred, cmap=lbl_cmap, alpha=0.35)\n",
    "            #im_path= '/nemo/stp/lm/working/fallest/StarDist_Course_Jan_2024/output_images/gray/'\n",
    "            filename_tiff = im_path+\"Labels_epoch_\"+str(epoch)+\"_\"+model.name+\".tiff\"\n",
    "            filename_png = im_path+\"Overlay_epoch_\"+str(epoch)+\"_\"+model.name+\".png\"\n",
    "            print(filename_tiff)\n",
    "            plt.imsave(filename_tiff, Y_val_pred, cmap=lbl_cmap)\n",
    "            plt.savefig(filename_png)\n",
    "            \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3d8c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set the image we want to use for the callback, and the path we want to save the images to\n",
    "validation_data = X_val[0]\n",
    "im_path= '/nemo/stp/lm/working/fallest/StarDist_Course_Jan_2024/output_images/gray/'\n",
    "\n",
    "if not os.path.exists(im_path):  #make the directory if it doesn't exist\n",
    "    os.makedirs(im_path)\n",
    "\n",
    "\n",
    "\n",
    "evaluate_callback = EvaluateModelCallback(validation_data, im_path) #set up the callback with the image and the path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbbdf2f5",
   "metadata": {},
   "source": [
    "### Show the image we're validating on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dfed5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(validation_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cddc9e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0756521c",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045b5133",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24406746",
   "metadata": {},
   "outputs": [],
   "source": [
    "quick_demo = False\n",
    "#if false!\n",
    "if quick_demo==False:\n",
    "    epochs=2000\n",
    "    steps=20\n",
    "\n",
    "if quick_demo:\n",
    "    print (\n",
    "        \"NOTE: This is only for a quick demonstration!\\n\"\n",
    "        \"      Please set the variable 'quick_demo = False' for proper (long) training.\",\n",
    "        file=sys.stderr, flush=True\n",
    "    )\n",
    "    model.train(X_trn, Y_trn, validation_data=(X_val,Y_val), augmenter=augmenter,\n",
    "                epochs=2, steps_per_epoch=10)\n",
    "\n",
    "    print(\"====> Stopping training and loading previously trained demo model from disk.\", file=sys.stderr, flush=True)\n",
    "    model = StarDist2D.from_pretrained('2D_demo')\n",
    "else:\n",
    "    model.prepare_for_training() # open up the model for adding callbacks\n",
    "    model.callbacks.append(evaluate_callback) #add our callback\n",
    "    model.train(X_trn, Y_trn, validation_data=(X_val,Y_val), augmenter=augmenter, epochs=epochs, steps_per_epoch=steps)\n",
    "None;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc1bab3",
   "metadata": {},
   "source": [
    "# Metrics:\n",
    "The code for the metrics can be found in : https://github.com/stardist/stardist/blob/master/stardist/models/base.py\n",
    "\n",
    "In the above computations, you'll see the same metric twice, i.e. loss and val_loss.  In this case, loss is the loss computed on the training set, and val_loss is the loss computed on the validation set.\n",
    "\n",
    "Some of the metrics that will help you understand what's going on are:\n",
    "\n",
    "* loss - Measure of how well the model is performing, the error between the predicted values and ground truth. We want this value to be low\n",
    "\n",
    "* prob_kld - Kullback-Leibler Divergence. This measures the divergence between two probability distributions, in this case the predicted probability distribution and the expected probability distributions. This should be low\n",
    "\n",
    "* iou_metric - The intersection over union.  The closer to 1 this is, the better\n",
    "\n",
    "* mae - Mean absolute error, difference between predicted and ground truth. This should decrease over time.\n",
    "\n",
    "* mse - Mean squared error of difference between predicted and ground truth. This should decrease over time.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105391f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if quick_demo:\n",
    "    model.optimize_thresholds(X_val[:2], Y_val[:2])\n",
    "else:\n",
    "    model.optimize_thresholds(X_val, Y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1fbeb3e",
   "metadata": {},
   "source": [
    "# Evaluation and Detection Performance\n",
    "Besides the losses and metrics during training, we can also quantitatively evaluate the actual detection/segmentation performance on the validation data by considering objects in the ground truth to be correctly matched if there are predicted objects with overlap (here [intersection over union (IoU)](https://en.wikipedia.org/wiki/Jaccard_index)) beyond a chosen IoU threshold $\\tau$.\n",
    "\n",
    "\n",
    "The corresponding matching statistics (average overlap, accuracy, recall, precision, etc.) are typically of greater practical relevance than the losses/metrics computed during training (but harder to formulate as a loss function). \n",
    "\n",
    "### Tau: Tau is the IoU threshold.  \n",
    "\n",
    "### The value of $\\tau$ can be between 0 (even slightly overlapping objects count as correctly predicted) and 1 (only pixel-perfectly overlapping objects count) and which $\\tau$ to use depends on the needed segmentation precision/application.\n",
    "\n",
    "Please see `help(matching)` for definitions of the abbreviations used in the evaluation below and see the Wikipedia page on [Sensitivity and specificity](https://en.wikipedia.org/wiki/Sensitivity_and_specificity) for further details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5193eb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#help(matching)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9e572ca",
   "metadata": {},
   "source": [
    "### First predict the labels for all validation images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d69ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_val_pred = [model.predict_instances(x, n_tiles=model._guess_n_tiles(x), show_tile_progress=False)[0]\n",
    "              for x in tqdm(X_val)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7c06fc",
   "metadata": {},
   "source": [
    "Plot a GT/prediction example "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a2dc76",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_img_label(X_val[0],Y_val[0], lbl_title=\"label GT\")\n",
    "plot_img_label(X_val[0],Y_val_pred[0], lbl_title=\"label Pred\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30cee9d4",
   "metadata": {},
   "source": [
    "### Choose several IoU thresholds $\\tau$ that might be of interest and for each compute matching statistics for the validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3c7245",
   "metadata": {},
   "outputs": [],
   "source": [
    "taus = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "stats = [matching_dataset(Y_val, Y_val_pred, thresh=t, show_progress=False) for t in tqdm(taus)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114b315c",
   "metadata": {},
   "source": [
    "## Example: Print all available matching statistics for $\\tau=0.5$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b858089",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats[taus.index(0.5)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e23d3a",
   "metadata": {},
   "source": [
    "### Plot the matching statistics and the number of true/false positives/negatives as a function of the IoU threshold $\\tau$. \n",
    "\n",
    "* False positives are the number of objects that are identified that aren't there in reality.\n",
    "* False negatives are the objects that are missed by the algorithm\n",
    "* True positives are the ones that are correct\n",
    "\n",
    "\n",
    "### As we increase tau, we tend to get many more false negatives and less true positives, as we are requiring the matches to be more and more perfect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60c791d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1,ax2) = plt.subplots(1,2, figsize=(15,5))\n",
    "\n",
    "for m in ('precision', 'recall', 'accuracy', 'f1', 'mean_true_score', 'mean_matched_score', 'panoptic_quality'):\n",
    "    ax1.plot(taus, [s._asdict()[m] for s in stats], '.-', lw=2, label=m)\n",
    "ax1.set_xlabel(r'IoU threshold $\\tau$')\n",
    "ax1.set_ylabel('Metric value')\n",
    "ax1.grid()\n",
    "ax1.legend()\n",
    "\n",
    "for m in ('fp', 'tp', 'fn'):\n",
    "    ax2.plot(taus, [s._asdict()[m] for s in stats], '.-', lw=2, label=m)\n",
    "ax2.set_xlabel(r'IoU threshold $\\tau$')\n",
    "ax2.set_ylabel('Number #')\n",
    "ax2.grid()\n",
    "ax2.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b2d2cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Local StarDist_Crick_Feb24",
   "language": "",
   "name": "rik_local_stardist_crick_feb24"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
